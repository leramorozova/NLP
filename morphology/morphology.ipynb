{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование TreeTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если TreeTagger не установлен, прогоните следующую ячейку (скрипт установки в папке с заданием):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh get_treetagger.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "Я\tP-1-snn\tя\n",
      "хочу\tVmip1s-a-e\tхотеть\n",
      "съесть\tVmn----a-p\tсъесть\n",
      "яблоко\tNcnsan\tяблоко\n",
      "!\tSENT\t!\n",
      "\t finished.\n"
     ]
    }
   ],
   "source": [
    "!echo 'Я хочу съесть яблоко!' | cmd/tree-tagger-russian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "open_corpora = etree.fromstring(open('annot.opcorpora.no_ambig_strict.xml', 'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого у нас 10590 предложений. В таком случае разделим выборку на 10 частей: в каждой итерации будет 1059 примеров на тестовой выборке и 9531 предложение в тренировочной. Создадим массив кортежей (форматированое предложение для обучение, стандартное предложение для теста, верный грамматический разбор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10590\n",
      "('«\\tPNCT\\nШкола\\tNOUN,inan,femn,sing,nomn\\nзлословия\\tNOUN,inan,neut,sing,gent\\n»\\tPNCT\\nучит\\tVERB,impf,tran,sing,3per,pres,indc\\nприкусить\\tINFN,perf,tran\\nязык\\tNOUN,inan,masc,sing,accs\\n.\\tSENT\\n', '« Школа злословия » учит прикусить язык', [['PNCT'], ['NOUN', 'inan', 'femn', 'sing', 'nomn'], ['NOUN', 'inan', 'neut', 'sing', 'gent'], ['PNCT'], ['VERB', 'impf', 'tran', 'sing', '3per', 'pres', 'indc'], ['INFN', 'perf', 'tran'], ['NOUN', 'inan', 'masc', 'sing', 'accs']])\n",
      "('Сохранится\\tVERB,perf,intr,sing,3per,futr,indc\\nли\\tPRCL\\nградус\\tNOUN,inan,masc,sing,nomn\\nдискуссии\\tNOUN,inan,femn,sing,gent\\nв\\tPREP\\nновом\\tADJF,Qual,masc,sing,loct\\nсезоне\\tNOUN,inan,masc,sing,loct\\n?\\tSENT\\n', 'Сохранится ли градус дискуссии в новом сезоне ?', [['VERB', 'perf', 'intr', 'sing', '3per', 'futr', 'indc'], ['PRCL'], ['NOUN', 'inan', 'masc', 'sing', 'nomn'], ['NOUN', 'inan', 'femn', 'sing', 'gent'], ['PREP'], ['ADJF', 'Qual', 'masc', 'sing', 'loct'], ['NOUN', 'inan', 'masc', 'sing', 'loct'], ['SENT']])\n"
     ]
    }
   ],
   "source": [
    "vocab = defaultdict(set)\n",
    "tags = set()\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in open_corpora.xpath('//tokens'):\n",
    "    formatted_sent = \"\"\n",
    "    standart_sent = []\n",
    "    gram_infos = []\n",
    "    length = len(sentence.xpath('token'))\n",
    "    ended = False\n",
    "    for i,token in enumerate(sentence.xpath('token')):\n",
    "        word = token.xpath('@text')\n",
    "        gram_info = token.xpath('tfr/v/l/g/@v')\n",
    "        if (i + 1) == length and gram_info[0] == 'PNCT':\n",
    "            gram_info = ['SENT']\n",
    "            ended = True\n",
    "        formatted_sent += word[0] + '\\t' + ','.join(gram_info) + '\\n'\n",
    "        standart_sent.append(word[0])\n",
    "        lemma = token.xpath('tfr/v/l/@t')[0]\n",
    "        vocab[word[0].lower()].add((','.join(gram_info), lemma.lower()))\n",
    "        tags.add(','.join(gram_info))\n",
    "        gram_infos.append(gram_info)\n",
    "    if not ended:\n",
    "        formatted_sent += '.\\tSENT\\n'\n",
    "    standart_sent = \" \".join(standart_sent)\n",
    "    corpus.append((formatted_sent, standart_sent, gram_infos))\n",
    "\n",
    "print(len(corpus))\n",
    "for i in range(2):\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('lexicon.txt', 'w')\n",
    "\n",
    "for word in vocab:\n",
    "    f.write(word + '\\t')\n",
    "    f.write('\\t'.join([' '.join(pair) for pair in vocab[word]]))\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "f = open('open_class.txt', 'w')\n",
    "\n",
    "f.write('\\n'.join([tag for tag in tags if 'NOUN' in tag or 'VERB' in tag or 'ADJF' in tag]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "cross_val_folds = []\n",
    "\n",
    "fold = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    fold.append(corpus[i])\n",
    "    if i % (len(corpus) / 10) == 0:\n",
    "        cross_val_folds.append(fold)\n",
    "        fold = []\n",
    "        \n",
    "print(len(cross_val_folds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для парсинга выхода модели: возвращает массив предсказанных частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_predicted_pos(filename):\n",
    "    with open(filename, 'r') as fd:\n",
    "        content = fd.read()\n",
    "    content = content.split(\"\\n\")\n",
    "    pos = []\n",
    "    for word in content:\n",
    "        if len(word) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            pos.append(word.split(',')[0])\n",
    "    return pos\n",
    "\n",
    "def get_true_pos(cross_val_fold):\n",
    "    pos = []\n",
    "    for sent in cross_val_fold:\n",
    "        for word in sent[2]:\n",
    "            pos.append(word[0])\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для определения ошибки: сравнивает предсказанный результат с истинным и возвращает массив из нулей и единиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def if_mistake(output_filename, cross_val_fold):\n",
    "    score = []\n",
    "    predicted = get_predicted_pos(output_filename)\n",
    "    true = get_true_pos(cross_val_fold)\n",
    "    for pos in range(len(predicted)):\n",
    "        try:\n",
    "            if predicted[pos] == true[pos]:\n",
    "                score.append(1)\n",
    "            else:\n",
    "                score.append(0)\n",
    "        except IndexError:\n",
    "            sys.exit(0)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "53000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1796 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "65627\t32360\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "86\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 87\n",
      "Max. path length: 15\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:03<00:35,  3.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "46000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1799 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "57000\t28980\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "86\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 87\n",
      "Max. path length: 15\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "7000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:08<00:32,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "46000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1796 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "57383\t28907\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "24"
     ]
    }
   ],
   "source": [
    "mistake_score = []\n",
    "\n",
    "for i in tqdm(range(len(cross_val_folds))):\n",
    "    !rm corpus_train.txt\n",
    "    !rm corpus_test.txt\n",
    "    fd_train = open('corpus_train.txt', 'w')\n",
    "    fd_test = open('corpus_test.txt', 'w')\n",
    "    j = len(cross_val_folds) - 1\n",
    "    while j != -1:\n",
    "        if j == i:\n",
    "            for sent in cross_val_folds[i]:\n",
    "                fd_test.write('\\n' + '\\n'.join(sent[1].split()))\n",
    "        else:\n",
    "            for sent in cross_val_folds[j]:\n",
    "                fd_train.write(sent[0])\n",
    "        j -= 1\n",
    "    !./bin/train-tree-tagger lexicon.txt open_class.txt corpus_train.txt model_oc\n",
    "    !./bin/tree-tagger model_oc corpus_test.txt output.txt\n",
    "    mistake_score.append(if_mistake(\"output.txt\", cross_val_folds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на последнем фолде, адекватно ли вообще выглядит выход. Вроде более-менее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Не смей ругать мою землю .\n",
      "\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "\t finished.\n",
      "PNCT\n",
      "NOUN,inan,masc,sing,nomn\n",
      "VERB,impf,intr,sing,impr,excl\n",
      "INFN,impf,tran\n",
      "ADJF,Apro,femn,sing,accs\n",
      "NOUN,inan,femn,sing,accs\n",
      "SENT\n"
     ]
    }
   ],
   "source": [
    "demo = open('demo.txt', 'w')\n",
    "\n",
    "print(cross_val_folds[len(cross_val_folds) - 1][0][1] + \"\\n\")\n",
    "demo.write('\\n' + '\\n'.join(cross_val_folds[len(cross_val_folds) - 1][0][1].split()))\n",
    "    \n",
    "!./bin/tree-tagger model_oc demo.txt output.txt\n",
    "\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усередним ошибку по каждому фолду и посмотрим на общую ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold # 1 \t score:  nan\n",
      "fold # 2 \t score:  0.8967030211859095\n",
      "fold # 3 \t score:  0.9005208333333333\n",
      "fold # 4 \t score:  0.9017787078210829\n",
      "fold # 5 \t score:  0.8982542991141219\n",
      "fold # 6 \t score:  0.8901786878831355\n",
      "fold # 7 \t score:  0.9148299748110831\n",
      "fold # 8 \t score:  0.8904109589041096\n",
      "fold # 9 \t score:  0.8724179829890644\n",
      "fold # 10 \t score:  0.8854296388542964\n",
      "\n",
      "Total score:  nan\n"
     ]
    }
   ],
   "source": [
    "folds_result = []\n",
    "\n",
    "for i in range(len(mistake_score)):\n",
    "    fold_result = np.mean(mistake_score[i])\n",
    "    print(\"fold #\", i + 1, \"\\t score: \", fold_result)\n",
    "    folds_result.append(fold_result)\n",
    "    \n",
    "print(\"\\nTotal score: \", np.mean(folds_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
