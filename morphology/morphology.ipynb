{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестирование TreeTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если TreeTagger не установлен, прогоните следующую ячейку (скрипт установки в папке с заданием):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sh get_treetagger.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "Я\tP-1-snn\tя\n",
      "хочу\tVmip1s-a-e\tхотеть\n",
      "съесть\tVmn----a-p\tсъесть\n",
      "яблоко\tNcnsan\tяблоко\n",
      "!\tSENT\t!\n",
      "\t finished.\n"
     ]
    }
   ],
   "source": [
    "!echo 'Я хочу съесть яблоко!' | cmd/tree-tagger-russian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "open_corpora = etree.fromstring(open('annot.opcorpora.no_ambig_strict.xml', 'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого у нас 10590 предложений. В таком случае разделим выборку на 10 частей: в каждой итерации будет 1059 примеров на тестовой выборке и 9531 предложение в тренировочной. Создадим массив кортежей (форматированое предложение для обучение, стандартное предложение для теста, верный грамматический разбор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10590\n",
      "('«\\tPNCT\\nШкола\\tNOUN,inan,femn,sing,nomn\\nзлословия\\tNOUN,inan,neut,sing,gent\\n»\\tPNCT\\nучит\\tVERB,impf,tran,sing,3per,pres,indc\\nприкусить\\tINFN,perf,tran\\nязык\\tNOUN,inan,masc,sing,accs\\n.\\tSENT\\n', '« Школа злословия » учит прикусить язык', [('«', ['PNCT']), ('Школа', ['NOUN', 'inan', 'femn', 'sing', 'nomn']), ('злословия', ['NOUN', 'inan', 'neut', 'sing', 'gent']), ('»', ['PNCT']), ('учит', ['VERB', 'impf', 'tran', 'sing', '3per', 'pres', 'indc']), ('прикусить', ['INFN', 'perf', 'tran']), ('язык', ['NOUN', 'inan', 'masc', 'sing', 'accs'])])\n",
      "('Сохранится\\tVERB,perf,intr,sing,3per,futr,indc\\nли\\tPRCL\\nградус\\tNOUN,inan,masc,sing,nomn\\nдискуссии\\tNOUN,inan,femn,sing,gent\\nв\\tPREP\\nновом\\tADJF,Qual,masc,sing,loct\\nсезоне\\tNOUN,inan,masc,sing,loct\\n?\\tSENT\\n', 'Сохранится ли градус дискуссии в новом сезоне ?', [('Сохранится', ['VERB', 'perf', 'intr', 'sing', '3per', 'futr', 'indc']), ('ли', ['PRCL']), ('градус', ['NOUN', 'inan', 'masc', 'sing', 'nomn']), ('дискуссии', ['NOUN', 'inan', 'femn', 'sing', 'gent']), ('в', ['PREP']), ('новом', ['ADJF', 'Qual', 'masc', 'sing', 'loct']), ('сезоне', ['NOUN', 'inan', 'masc', 'sing', 'loct']), ('?', ['SENT'])])\n"
     ]
    }
   ],
   "source": [
    "vocab = defaultdict(set)\n",
    "tags = set()\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for sentence in open_corpora.xpath('//tokens'):\n",
    "    formatted_sent = \"\"\n",
    "    standart_sent = []\n",
    "    gram_infos = []\n",
    "    length = len(sentence.xpath('token'))\n",
    "    ended = False\n",
    "    for i,token in enumerate(sentence.xpath('token')):\n",
    "        word = token.xpath('@text')\n",
    "        gram_info = token.xpath('tfr/v/l/g/@v')\n",
    "        if (i + 1) == length and gram_info[0] == 'PNCT':\n",
    "            gram_info = ['SENT']\n",
    "            ended = True\n",
    "        formatted_sent += word[0] + '\\t' + ','.join(gram_info) + '\\n'\n",
    "        standart_sent.append(word[0])\n",
    "        lemma = token.xpath('tfr/v/l/@t')[0]\n",
    "        vocab[word[0].lower()].add((','.join(gram_info), lemma.lower()))\n",
    "        tags.add(','.join(gram_info))\n",
    "        gram_infos.append((word[0], gram_info))\n",
    "    if not ended:\n",
    "        formatted_sent += '.\\tSENT\\n'\n",
    "    standart_sent = \" \".join(standart_sent)\n",
    "    corpus.append((formatted_sent, standart_sent, gram_infos))\n",
    "\n",
    "print(len(corpus))\n",
    "for i in range(2):\n",
    "    print(corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('lexicon.txt', 'w')\n",
    "\n",
    "for word in vocab:\n",
    "    f.write(word + '\\t')\n",
    "    f.write('\\t'.join([' '.join(pair) for pair in vocab[word]]))\n",
    "    f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "f = open('open_class.txt', 'w')\n",
    "\n",
    "f.write('\\n'.join([tag for tag in tags if 'NOUN' in tag or 'VERB' in tag or 'ADJF' in tag]))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "cross_val_folds = []\n",
    "\n",
    "fold = []\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    fold.append(corpus[i])\n",
    "    if i % (len(corpus) / 10) == 0:\n",
    "        cross_val_folds.append(fold)\n",
    "        fold = []\n",
    "        \n",
    "print(len(cross_val_folds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для парсинга выхода модели: возвращает массив предсказанных частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_predicted_pos(filename):\n",
    "    with open(filename, 'r') as fd:\n",
    "        content = fd.read()\n",
    "    content = content.split(\"\\n\")\n",
    "    pos = []\n",
    "    for word in content:\n",
    "        if len(word) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            pos.append(word.split(',')[0])\n",
    "    return pos\n",
    "\n",
    "def get_true_pos(cross_val_fold):\n",
    "    pos = []\n",
    "    for sent in cross_val_fold:\n",
    "        for word in sent[2]:\n",
    "            pos.append((word[0], word[1][0]))\n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для определения ошибки: сравнивает предсказанный результат с истинным и возвращает массив из нулей и единиц."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import Counter\n",
    "\n",
    "mistakes = Counter()\n",
    "\n",
    "def if_mistake(output_filename, cross_val_fold):\n",
    "    fold_mistake = Counter()\n",
    "    score = []\n",
    "    predicted = get_predicted_pos(output_filename)\n",
    "    true = get_true_pos(cross_val_fold)\n",
    "    for pos in range(len(predicted)):\n",
    "        try:\n",
    "            if predicted[pos] == true[pos][1]:\n",
    "                score.append(1)\n",
    "            else:\n",
    "                global mistakes\n",
    "                mistakes.update([(true[pos][0], true[pos][1], predicted[pos])])\n",
    "                fold_mistake.update([(true[pos][0], true[pos][1], predicted[pos])])\n",
    "                score.append(0)\n",
    "        except IndexError:\n",
    "            sys.exit(0)\n",
    "    return (score, fold_mistake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "53000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1796 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "65627\t32360\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "86\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 87\n",
      "Max. path length: 15\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|█         | 1/10 [00:03<00:33,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "46000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1799 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "57000\t28980\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "86\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 87\n",
      "Max. path length: 15\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "7000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 2/10 [00:08<00:31,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "46000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1796 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "57383\t28907\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "90\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 91\n",
      "Max. path length: 16\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "7000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|███       | 3/10 [00:11<00:26,  3.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "46000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1796 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "57461\t28792\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "86\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 87\n",
      "Max. path length: 15\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "7000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 4/10 [00:16<00:24,  4.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "46000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1793 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "57290\t28718\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "94\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 95\n",
      "Max. path length: 16\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "7000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 5/10 [00:21<00:22,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "46000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1797 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "57574\t29328\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "88\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 89\n",
      "Max. path length: 14\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "7000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 6/10 [00:25<00:17,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "47000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1795 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "58850\t30259\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "70\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 71\n",
      "Max. path length: 15\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "6000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|███████   | 7/10 [00:29<00:12,  4.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "48000\tmaking affix tree ...\n",
      "prefix lexicon: 806 nodes\n",
      "suffix lexicon: 1795 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "59098\t30045\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "90\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 91\n",
      "Max. path length: 15\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "5000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 8/10 [00:33<00:08,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "49000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1798 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "60215\t30625\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "82\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 83\n",
      "Max. path length: 14\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "4000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████ | 9/10 [00:38<00:04,  4.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train-tree-tagger -cl 2 -dtg 0.50 -sw 1.00 -ecw 0.15 -atg 1.20 lexicon.txt open_class.txt corpus_train.txt model_oc\n",
      "\n",
      "\treading the lexicon ...\n",
      "\t\treading the tagset ...\n",
      "\t\treading the lemmas ...\n",
      "\t\treading the entries ...\n",
      "\t\tsorting the lexicon ...\n",
      "\t\treading the open class tags ...\n",
      "\tcalculating tag frequencies ...\n",
      "48000\tmaking affix tree ...\n",
      "prefix lexicon: 807 nodes\n",
      "suffix lexicon: 1799 nodes\n",
      "\treading classes ...\n",
      "\tmaking ngram table ...\n",
      "59143\t29956\n",
      "finished.\n",
      "\tmaking decision tree ...\n",
      "92\tsaving parameters ...\n",
      "\n",
      "Number of nodes: 93\n",
      "Max. path length: 18\n",
      "\n",
      "done.\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "5000\t finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:43<00:00,  4.51s/it]\n"
     ]
    }
   ],
   "source": [
    "mistake_score = []\n",
    "\n",
    "for i in tqdm(range(len(cross_val_folds))):\n",
    "    !rm corpus_train.txt\n",
    "    !rm corpus_test.txt\n",
    "    fd_train = open('corpus_train.txt', 'w')\n",
    "    fd_test = open('corpus_test.txt', 'w')\n",
    "    j = len(cross_val_folds) - 1\n",
    "    while j != -1:\n",
    "        if j == i:\n",
    "            for sent in cross_val_folds[i]:\n",
    "                fd_test.write('\\n' + '\\n'.join(sent[1].split()))\n",
    "        else:\n",
    "            for sent in cross_val_folds[j]:\n",
    "                fd_train.write(sent[0])\n",
    "        j -= 1\n",
    "    !./bin/train-tree-tagger lexicon.txt open_class.txt corpus_train.txt model_oc\n",
    "    !./bin/tree-tagger model_oc corpus_test.txt output.txt\n",
    "    mistake_score.append(if_mistake(\"output.txt\", cross_val_folds[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на последнем фолде, адекватно ли вообще выглядит выход. Вроде более-менее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Не смей ругать мою землю .\n",
      "\n",
      "\treading parameters ...\n",
      "\ttagging ...\n",
      "\t finished.\n",
      "PNCT\n",
      "NOUN,inan,masc,sing,nomn\n",
      "VERB,impf,intr,sing,impr,excl\n",
      "INFN,impf,tran\n",
      "ADJF,Apro,femn,sing,accs\n",
      "NOUN,inan,femn,sing,accs\n",
      "SENT\n"
     ]
    }
   ],
   "source": [
    "demo = open('demo.txt', 'w')\n",
    "\n",
    "print(cross_val_folds[len(cross_val_folds) - 1][0][1] + \"\\n\")\n",
    "demo.write('\\n' + '\\n'.join(cross_val_folds[len(cross_val_folds) - 1][0][1].split()))\n",
    "    \n",
    "!./bin/tree-tagger model_oc demo.txt output.txt\n",
    "\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Усередним ошибку по каждому фолду и посмотрим на общую ошибку. Вроде ничего так."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold # 1 \t score:  0.8967030211859095\n",
      "fold # 2 \t score:  0.9005208333333333\n",
      "fold # 3 \t score:  0.9017787078210829\n",
      "fold # 4 \t score:  0.8982542991141219\n",
      "fold # 5 \t score:  0.8901786878831355\n",
      "fold # 6 \t score:  0.9148299748110831\n",
      "fold # 7 \t score:  0.8904109589041096\n",
      "fold # 8 \t score:  0.8724179829890644\n",
      "fold # 9 \t score:  0.8854296388542964\n",
      "\n",
      "Total score:  0.8942276354637784\n",
      "Std score:  0.011839130678523721\n"
     ]
    }
   ],
   "source": [
    "folds_result = []\n",
    "\n",
    "for i in range(1, len(mistake_score)):\n",
    "    fold_result = np.mean(mistake_score[i][0])\n",
    "    print(\"fold #\", i, \"\\t score: \", fold_result)\n",
    "    folds_result.append(fold_result)\n",
    "    \n",
    "print(\"\\nTotal score: \", np.mean(folds_result[1:]))\n",
    "print(\"Std score: \", np.std(folds_result[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем картина по ошибкам следующая: проблемы с определением, последний ли знак препинания в предложении (?) и почему-то местоимения и предлоги - это существительные, а не что-то еще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('В', 'PREP', 'NOUN'), 413),\n",
       " (('.', 'PNCT', 'SENT'), 324),\n",
       " (('Я', 'NPRO', 'NOUN'), 174),\n",
       " ((',', 'SENT', 'PNCT'), 124),\n",
       " (('Не', 'PRCL', 'NOUN'), 109),\n",
       " (('На', 'PREP', 'NOUN'), 93),\n",
       " ((':', 'PNCT', 'SENT'), 91),\n",
       " (('Он', 'NPRO', 'NOUN'), 85),\n",
       " (('»', 'SENT', 'PNCT'), 78),\n",
       " (('С', 'PREP', 'NOUN'), 71)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В среднем по фолдам ситуация одинакова, не очень понятно, почему так. Существуют ли вообще теги для местоимений и, союзов и предлогов?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold # 1\n",
      "(('.', 'PNCT', 'SENT'), 61)\n",
      "(('В', 'PREP', 'NOUN'), 43)\n",
      "((':', 'PNCT', 'SENT'), 23)\n",
      "(('»', 'SENT', 'PNCT'), 16)\n",
      "(('Не', 'PRCL', 'NOUN'), 16)\n",
      "\n",
      "\n",
      "fold # 2\n",
      "(('В', 'PREP', 'NOUN'), 68)\n",
      "(('Я', 'NPRO', 'NOUN'), 15)\n",
      "(('И', 'CONJ', 'NOUN'), 14)\n",
      "(('.', 'PNCT', 'SENT'), 14)\n",
      "(('Мы', 'NPRO', 'NOUN'), 14)\n",
      "\n",
      "\n",
      "fold # 3\n",
      "(('В', 'PREP', 'NOUN'), 47)\n",
      "(('.', 'PNCT', 'SENT'), 39)\n",
      "(('Я', 'NPRO', 'NOUN'), 24)\n",
      "(('На', 'PREP', 'NOUN'), 17)\n",
      "(('Мы', 'NPRO', 'NOUN'), 15)\n",
      "\n",
      "\n",
      "fold # 4\n",
      "(('В', 'PREP', 'NOUN'), 60)\n",
      "(('Я', 'NPRO', 'NOUN'), 30)\n",
      "(('Мы', 'NPRO', 'NOUN'), 13)\n",
      "((',', 'SENT', 'PNCT'), 13)\n",
      "(('Не', 'PRCL', 'NOUN'), 13)\n",
      "\n",
      "\n",
      "fold # 5\n",
      "(('.', 'PNCT', 'SENT'), 65)\n",
      "(('В', 'PREP', 'NOUN'), 43)\n",
      "(('Я', 'NPRO', 'NOUN'), 27)\n",
      "((')', 'SENT', 'PNCT'), 24)\n",
      "((',', 'SENT', 'PNCT'), 15)\n",
      "\n",
      "\n",
      "fold # 6\n",
      "(('.', 'PNCT', 'SENT'), 40)\n",
      "(('В', 'PREP', 'NOUN'), 33)\n",
      "(('.', 'SENT', 'PNCT'), 31)\n",
      "((',', 'SENT', 'PNCT'), 19)\n",
      "(('Российской', 'ADJF', 'NOUN'), 16)\n",
      "\n",
      "\n",
      "fold # 7\n",
      "((',', 'SENT', 'PNCT'), 38)\n",
      "(('.', 'PNCT', 'SENT'), 38)\n",
      "(('В', 'PREP', 'NOUN'), 32)\n",
      "(('Я', 'NPRO', 'NOUN'), 18)\n",
      "(('Не', 'PRCL', 'NOUN'), 12)\n",
      "\n",
      "\n",
      "fold # 8\n",
      "(('В', 'PREP', 'NOUN'), 40)\n",
      "(('.', 'PNCT', 'SENT'), 33)\n",
      "(('Я', 'NPRO', 'NOUN'), 25)\n",
      "(('Архивировано', 'PRTS', 'NOUN'), 24)\n",
      "(('Проверено', 'PRTS', 'NOUN'), 22)\n",
      "\n",
      "\n",
      "fold # 9\n",
      "(('В', 'PREP', 'NOUN'), 47)\n",
      "(('.', 'PNCT', 'SENT'), 27)\n",
      "(('Не', 'PRCL', 'NOUN'), 17)\n",
      "(('Он', 'NPRO', 'NOUN'), 16)\n",
      "((',', 'SENT', 'PNCT'), 13)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, len(mistake_score)):\n",
    "    print(\"fold #\", i)\n",
    "    for el in mistake_score[i][1].most_common(5):\n",
    "        print(el)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NOUN'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "morph.tag(\"слово\")[0].POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm2_mistakes = Counter()\n",
    "score = []\n",
    "\n",
    "for el in cross_val_folds[9]:\n",
    "    sent = el[2]\n",
    "    for word in sent:\n",
    "        if word[1][0] == 'PNCT' or word[1][0] == 'SENT':\n",
    "            pass\n",
    "        # пропустим определение знаков препинания\n",
    "        else:\n",
    "            analysis = morph.tag(word[0])[0].POS\n",
    "            if analysis == word[1][0]:\n",
    "                score.append(1)\n",
    "            else:\n",
    "                score.append(0)\n",
    "                pm2_mistakes.update([(word[0], analysis, word[1][0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pymorphy score:  0.8912466843501327\n"
     ]
    }
   ],
   "source": [
    "print(\"Pymorphy score: \", np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('1', None, 'NUMB'), 12),\n",
       " (('также', 'CONJ', 'PRCL'), 9),\n",
       " (('2', None, 'NUMB'), 9),\n",
       " (('a', None, 'LATN'), 8),\n",
       " (('3', None, 'NUMB'), 8)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm2_mistakes.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpencorporaTag('NUMB,intg')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('1')[0].tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество у Pymorphy, скорее всего, выше тритеггера - эти штуки, которые посчитались неправильно, на самом деле, опознаны верно, просто часть речи оттуда так просто не достать. Просто у такого рода входа не парсится часть речи как отдельный атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
