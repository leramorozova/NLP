{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тексты взяты с новостного сайта, раздела технологии. Каждая статья включала в себя теги по ключевым словам\n",
    "[https://politexpert.net/](https://politexpert.net/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "morph = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Article:\n",
    "    def __init__(self, text, keywords, my_keywords):\n",
    "        self.keywords = keywords.split(\",\")\n",
    "        self.my_keywords = my_keywords.split(\",\")\n",
    "        self.model_keywords = None\n",
    "        self.text = text\n",
    "        self.tokenized_text = self.parse_text(text)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"\\tkeywords: {self.keywords}\\n\\ttext: {self.text}\\n\\ttokens: {self.tokenized_text}\"\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_text(input_text, split_text=False):\n",
    "        if split_text:\n",
    "            return [morph.lemmatize(token)[0] for token in input_text \n",
    "                    if token not in stopwords and token not in punctuation]\n",
    "        return \" \".join([morph.lemmatize(token)[0] for token in nltk.word_tokenize(input_text)\n",
    "            if token not in stopwords and token not in punctuation])\n",
    "    \n",
    "    def count_metrics(self, my_keywords=False):\n",
    "        if my_keywords:\n",
    "            pres = len(set(self.model_keywords) & set(self.my_keywords)) / len(self.model_keywords)\n",
    "            rec = len(set(self.model_keywords) & set(self.my_keywords)) / len(self.my_keywords)\n",
    "            f1 = 2 * pres * rec / (pres + rec) if rec != 0 and pres != 0 else 0 \n",
    "        else:\n",
    "            pres = len(set(self.model_keywords) & set(self.keywords)) / len(self.model_keywords)\n",
    "            rec = len(set(self.model_keywords) & set(self.keywords)) / len(self.keywords)\n",
    "            f1 = 2 * pres * rec / (pres + rec) if rec != 0 and pres != 0 else 0 \n",
    "        return pres, rec, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'я очень уставать хотеть спать отпускать пожалуйста'"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text = \"Я очень устала и хочу спать, отпустите меня, пожалуйста!\"\n",
    "\n",
    "Article.parse_text(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(os.path.join(\"corpus.txt\"), \"r\") as fd:\n",
    "    corpus = fd.read().split(\"\\n\")\n",
    "\n",
    "corpus = [Article(corpus[i + 2], corpus[i], corpus[i + 1]) for i in range(0, len(corpus) - 1, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\tkeywords: ['новинка', 'xiaomi', 'устройство', 'технологии']\n",
       "\ttext: Китайская компания Xiaomi выпустила оригинальное зарядное устройство, которое сможет подогревать кофе и одновременно заряжать смартфон. Об этом пишет портал ixbt.com. Новое беспроводное устройство от Xiaomi представляет собой чашку с подставкой, которая способна поддерживать температуру до +55 градусов. Создатели гаджета утверждают, что их метод нагревания совершенно безопасен и стабилен, в отличие от проводного. Они рассказали, что чашка сделана из прочной керамики и ничем не отличается от обычной посуды. Ее можно спокойно мыть и суть. Кружка начинает нагреваться при помещении на подставку. Спустя четыре часа устройство самостоятельно выключается, если его не использовать. Также на подставке можно не только подогревать напитки, но и заряжать смартфон. Устройство также выполняет функцию беспроводного зарядного устройства для гаджетов от Xiaomi, Apple, Huawei и Samsung. Стоимость оригинального девайса составляет около 1500 рублей. Ранее Xiaomi представила свои первые смарт-часы Mi Watch. Они стоят примерно 12 тысяч рублей.\n",
       "\ttokens: китайский компания Xiaomi выпускать оригинальный зарядный устройство который смочь подогревать кофе одновременно заряжать смартфон об писать портал ixbt новое беспроводный устройство Xiaomi представлять себя чашка подставка который способный поддерживать температура + градус создатель гаджет утверждать метод нагревание совершенно безопасный стабильный отличие проводной они рассказывать чашка сделать прочный керамика ничто отличаться обычный посуда она спокойно мыть суть кружка начинать нагреваться помещение подставка спустя четыре час устройство самостоятельно выключаться использовать также подставка подогревать напиток заряжать смартфон устройство также выполнять функция беспроводный зарядный устройство гаджет Xiaomi Apple Huawei Samsung стоимость оригинальный девайса составлять около 1500 рубль ранее Xiaomi представлять свой первый смарт Mi Watch они стоять примерно 12 тысяча рубль"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть кейвордов сходится, потому что иногда я была согласна с тегами, встроенными в статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст 0\n",
      "{'xiaomi', 'устройство'}\n",
      "Текст 1\n",
      "{'apple', 'курение'}\n",
      "Текст 2\n",
      "{'цензура', 'instagram', 'facebook'}\n",
      "Текст 3\n",
      "{'молния'}\n",
      "Текст 4\n",
      "{'apple'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print(f\"Текст {i}\")\n",
    "    print(set(corpus[i].keywords) & set(corpus[i].my_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но я делала теги более подробными + пыталась брать только те слова, которые встречаются в самом тексте. Поэтому есть некоторая разница."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст 0\n",
      "Есть у меня, нет в оригинале: {'бепровоное', 'чашка'}\n",
      "Есть в оригинале, нет у меня, нет в оригинале: {'технологии', 'новинка'}\n",
      "Текст 1\n",
      "Есть у меня, нет в оригинале: {'приложение', 'вейп'}\n",
      "Есть в оригинале, нет у меня, нет в оригинале: {'сигареты', 'электронные', 'вейпы'}\n",
      "Текст 2\n",
      "Есть у меня, нет в оригинале: {'законопроект'}\n",
      "Есть в оригинале, нет у меня, нет в оригинале: set()\n",
      "Текст 3\n",
      "Есть у меня, нет в оригинале: {'удар', 'расчет', 'анализ', 'система'}\n",
      "Есть в оригинале, нет у меня, нет в оригинале: {'технологии', 'искусственный интеллект'}\n",
      "Текст 4\n",
      "Есть у меня, нет в оригинале: {'телефон', 'iphone', 'интернет'}\n",
      "Есть в оригинале, нет у меня, нет в оригинале: {'обновление', 'смартфон'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    print(f\"Текст {i}\")\n",
    "    print(\"Есть у меня, нет в оригинале:\", set(corpus[i].my_keywords) - set(corpus[i].keywords))\n",
    "    print(\"Есть в оригинале, нет у меня, нет в оригинале:\", set(corpus[i].keywords) - set(corpus[i].my_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer()\n",
    "tf_idf_matrix = tf_idf.fit_transform([text.tokenized_text for text in corpus]).T.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оригинал: ['новинка', 'xiaomi', 'устройство', 'технологии'] \n",
      "модель: ['устройство', 'xiaomi', 'подставка', 'беспроводный', 'гаджет'] \n",
      "\n",
      "оригинал: ['электронные', 'сигареты', 'apple', 'курение', 'вейпы'] \n",
      "модель: ['программа', 'курение', 'приложение', 'удалять', 'app'] \n",
      "\n",
      "оригинал: ['facebook', 'instagram', 'цензура'] \n",
      "модель: ['платформа', 'facebook', 'социальный', 'аккаунт', 'блогер'] \n",
      "\n",
      "оригинал: ['молния', 'технологии', 'искусственный интеллект'] \n",
      "модель: ['изобретение', 'молния', 'точность', 'информация', '30'] \n",
      "\n",
      "оригинал: ['apple', 'смартфон', 'обновление'] \n",
      "модель: ['iphone', 'владелец', 'модель', 'девайс', 'интернет'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_table = pd.DataFrame(data=tf_idf_matrix, index=tf_idf.get_feature_names())\n",
    "\n",
    "for i in range(tf_idf_matrix.shape[1]):\n",
    "    corpus[i].model_keywords = list(result_table[i].nlargest(5).index)\n",
    "    print(\"оригинал:\", corpus[i].keywords, \"\\nмодель:\", corpus[i].model_keywords, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pres, rec, f1 = np.mean([corpus[i].count_metrics() for i in range(tf_idf_matrix.shape[1])], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение с оригинальными тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.2\n",
      "recall: 0.2733333333333333\n",
      "f1: 0.22888888888888886\n"
     ]
    }
   ],
   "source": [
    "print(\"precision:\", pres)\n",
    "print(\"recall:\", rec)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение с моими тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.32\n",
      "recall: 0.39\n",
      "f1: 0.3511111111111111\n"
     ]
    }
   ],
   "source": [
    "pres, rec, f1 = np.mean([corpus[i].count_metrics(my_keywords=True) \n",
    "                         for i in range(tf_idf_matrix.shape[1])], axis=0)\n",
    "print(\"precision:\", pres)\n",
    "print(\"recall:\", rec)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оригинал: ['новинка', 'xiaomi', 'устройство', 'технологии'] \n",
      "модель: ['устроиство', 'xiaomi', 'чашка подставка', 'они', 'оригинальныи', 'гаджет', 'подогревать', 'беспроводныи', 'также', 'которыи', 'рубль', 'представлять', 'заряжать', 'портал', 'первыи'] \n",
      "\n",
      "оригинал: ['электронные', 'сигареты', 'apple', 'курение', 'вейпы'] \n",
      "модель: ['которыи', 'курение', 'приложение', 'программа', 'apple', 'удалять', 'корпорация', 'веип', 'это', 'app', 'яблочныи', 'компания'] \n",
      "\n",
      "оригинал: ['facebook', 'instagram', 'цензура'] \n",
      "модель: ['facebook', 'блогер получать платформа', 'издание', 'подвергаться', 'видео', 'аккаунт', 'социальныи сеть', 'свои', 'компания', 'это', 'политическии цензура', 'юридическии лицо', 'новость тема кликать', 'несоответствие', 'instagram', 'ценность'] \n",
      "\n",
      "оригинал: ['молния', 'технологии', 'искусственный интеллект'] \n",
      "модель: ['молния', 'точность', 'изобретение', 'информация', 'лаборатория', 'территория', 'оперныи', 'сиднеискии', 'электромагнитныи совместимость', 'начинать гроза уточнять', 'знаменитыи', 'техническии'] \n",
      "\n",
      "оригинал: ['apple', 'смартфон', 'обновление'] \n",
      "модель: ['iphone', 'интернет', 'также', 'оставаться', 'деваис', 'американскии', 'старыи модель', 'ноябрь', 'новыи', 'владелец', 'неудобно', 'смс перед', 'которыи'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import keywords\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i].model_keywords = keywords(corpus[i].tokenized_text).split('\\n')\n",
    "    print(\"оригинал:\", corpus[i].keywords, \"\\nмодель:\", corpus[i].model_keywords, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Куда он дел й??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение с оригинальными тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.08833333333333333\n",
      "recall: 0.32999999999999996\n",
      "f1: 0.13688338493292052\n"
     ]
    }
   ],
   "source": [
    "pres, rec, f1 = np.mean([corpus[i].count_metrics() for i in range(tf_idf_matrix.shape[1])], axis=0)\n",
    "\n",
    "print(\"precision:\", pres)\n",
    "print(\"recall:\", rec)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение с моими тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.13576923076923078\n",
      "recall: 0.44000000000000006\n",
      "f1: 0.20664086687306504\n"
     ]
    }
   ],
   "source": [
    "pres, rec, f1 = np.mean([corpus[i].count_metrics(my_keywords=True) \n",
    "                         for i in range(tf_idf_matrix.shape[1])], axis=0)\n",
    "print(\"precision:\", pres)\n",
    "print(\"recall:\", rec)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RAKE\n",
    "\n",
    "rk = RAKE.Rake(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На нетокенизированном тексте на проверку извлечение ключевых слов оказалось лучше, поэтому на вход модели подаю его. Далее токенизирую полученные слова самостоятельно для подсчета метрик."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "оригинал: ['новинка', 'xiaomi', 'устройство', 'технологии'] \n",
      "модель: ['com', 'подставка', 'стабильный', 'отличие', 'проводной', 'рассказывать', 'ничто', 'отличаться', 'суть', 'помещение', 'подставка', 'использовать', 'также', 'подставка', 'гаджет', 'xiaomi', 'apple', 'huawei', 'samsung'] \n",
      "\n",
      "оригинал: ['электронные', 'сигареты', 'apple', 'курение', 'вейпы'] \n",
      "модель: ['приложение', 'вейпинг', 'это', 'например', 'некоторые', 'кроме', 'игра', 'который'] \n",
      "\n",
      "оригинал: ['facebook', 'instagram', 'цензура'] \n",
      "модель: ['получать', 'видео', 'igtv', 'ссылка', 'bloomberg', 'говорить', 'выборы', 'политика', 'платформа', 'предлагать', 'неудивительный', 'несоответствие', 'идеология', 'россия', 'африка', 'очевидно', 'другой', 'никто', 'нравиться', 'фзнц', 'госдума', 'хотеть', 'новость', 'тема', 'кликать', 'подписываться', 'яндекс'] \n",
      "\n",
      "оригинал: ['молния', 'технологии', 'искусственный интеллект'] \n",
      "модель: ['фотограф', 'точность', 'проводить', 'анализ', 'разработка', 'соответственно', 'функционировать', 'данные', 'влажность', 'ветер', 'ударять'] \n",
      "\n",
      "оригинал: ['apple', 'смартфон', 'обновление'] \n",
      "модель: ['интернет', 'ru', 'iphone', 'который', 'обновлять', 'сталкиваться', 'проблема', 'заходить', 'интернет', 'использовать', 'принимать', 'обновление', 'подчеркивать'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i].model_keywords = Article.parse_text([k[0] for k in \n",
    "                                                   rk.run(corpus[i].text, maxWords=1)], split_text=True)\n",
    "    print(\"оригинал:\", corpus[i].keywords, \"\\nмодель:\", corpus[i].model_keywords, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение с оригинальными тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.025910931174089068\n",
      "recall: 0.11666666666666665\n",
      "f1: 0.042391304347826085\n"
     ]
    }
   ],
   "source": [
    "pres, rec, f1 = np.mean([corpus[i].count_metrics() for i in range(tf_idf_matrix.shape[1])], axis=0)\n",
    "\n",
    "print(\"precision:\", pres)\n",
    "print(\"recall:\", rec)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение с моими тегами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.08447736474052263\n",
      "recall: 0.24\n",
      "f1: 0.1227834612105712\n"
     ]
    }
   ],
   "source": [
    "pres, rec, f1 = np.mean([corpus[i].count_metrics(my_keywords=True) \n",
    "                         for i in range(tf_idf_matrix.shape[1])], axis=0)\n",
    "print(\"precision:\", pres)\n",
    "print(\"recall:\", rec)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Метрики очень плохие, чему я могу предположить ряд причин:\n",
    "1. Оригинальных ключевых слов маловато.\n",
    "2. Выбранные авторами ключевые слова могут не содержаться в самом тексте напрямую (например, ключевое слово \"смартфон\" в статье про айфон), поэтому данные алгоритмы не могут их отгадать, т.к. они опираются только на содержание текста.\n",
    "3. Объем статей маловат.\n",
    "\n",
    "В качестве решения могу предложить только расширение списка ключевых слов с учетом содержания статей (т.е., не использовать слова, которых нет в тексте). Это я и попробовала сделать, считая метрики на своих списках ключевых слов, и как можно увидеть, метрики действительно значительно подросли."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
